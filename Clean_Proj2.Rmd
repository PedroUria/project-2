---
title: "Oscars... WHAT?"
authors: Pedro Uria, Sean Pili and Zachary Buckley 
date: "12/06/2018"
output:
  html_document:
    toc: true
    theme: united
    highlight: tango
    number_sections: true
---

```{r setup, include=FALSE}
# !diagnostics off
knitr::opts_chunk$set(echo = TRUE)


```

# Stating the Question

TODO: Revise to Question-first mind-set

## Objectives

The goal of this project is to develop models to predict whether a movie will win some kind of Oscar award or not, and potentially .....(TODO). In order to achieve this, we will use a dataset that [BigMl](https://bigml.com/) [put together](https://bigml.com/user/academy_awards/gallery/dataset/5a94302592fb565ed400103b) and used to train a deep neural network to get [perfect predictions](https://blog.bigml.com/2018/03/01/predicting-the-2018-oscar-winners/) for the 8 categories they targeted for 2018. However, this does not mean their model is perfect, as ..... (TODO: source for where they say even they were surprised?).

## Motivation

The motivation is to use these models to identify the most relevant features that make a movie win an Academy Award, and also to have an edge when betting for the next Oscars winners. TODO: more motivations...?

## The Data

Gathering the data was very straightforward in this case. After coming up with the idea for the project, a simple Google search of: *oscars machine learning* almost directly handed us the dataset. We quickly found [this article](https://medium.com/enrique-dans/and-this-years-oscar-goes-to-bigml-machine-learning-1823837ae3aa), which lead us to BigML. After signing up on their website, we were able to download the dataset for free. However, this would not mean that the data would be ready to be fed to our machine learning algorithms right away... which we easily realized after doing some EDA. BigML combined data from IMDB with entries that specify whether a movie has won some other awards previous to the Oscars.

# Exploratory Data Analysis (EDA)

```{r install_functions, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
##Helper Functions here for cleanly handling imports
#install_all - function for installing all packages in a list that aren't available
#  inspired by https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them
#  ex: install_all(c('readr', 'dplyr'))
install_all <- function(list.of.packages) {
  need_install <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
  if(length(need_install)) install.packages(need_install)
}

# package_apply_all - function for calling install_all on list of packages, then applying a function to them
#  inspired by https://stackoverflow.com/questions/8175912/load-multiple-packages-at-once
#  ex: package_apply_all(c('readr', 'dplyr'), require)
package_apply_all <- function(list.of.packages, func) {
  install_all(list.of.packages)
  invisible(lapply(list.of.packages, func, character.only = TRUE))
}
```

```{r imports, include=FALSE}
package_apply_all(
  c(
    'readr', # for loading data from various formats
    'ggplot2', # pretty graphs
    'tidyr', # needed for the gather function
    'caret', # needed for standardizing
    'pROC', # needed for ROC curve
    'dplyr', # because we all need a little magic
    'import',
    'rBayesianOptimization', # used in cross-validation of step-wise logistic
    'car'
  ), library)
install_all(c(
  'tidyimpute',
  'caret'
))
import::from(tidyimpute, "impute_mean")
import::from(caret, "confusionMatrix")

```

## First Contact

Let's load the data and see what features we have and how large is our dataset

```{r}
oscars <- read_csv("oscars.csv", col_types = cols())
#View(head(oscars)) # Go to the .Rdm file and uncomment this line
# if you want to have a look at the data
dim(oscars)
#sapply(oscars, typeof) # Same here
```

So we have 1183 rows and 119 features. `year` will be useful to make a train-test split later, but we will drop it after that. `movie` and `movie_id` will also be dropped as they are not relevant for prediction. We will need to encode `certificate`, while `duration` is already in a suitable form, and `genre` will have to be converted to dummy variables. `rate` and `metascore` are also fine. We will also drop `synopsis`, which is very unlikely to matter significantly, and which would also require Natural Language Processing, as skill we lack? (TODO: maybe delete the NLP part). `votes` is also fine, while `gross` will need to be modified to account for inflation over the years. We will also drop `release_date` but will use `release_date.month` (at the end of the features) to create a new column `season`, which is known to be a relevant factor in winning an Oscar. `release_date.year`, `release_date.day-of-month` and `release_date.day-of-week` will also be dropped. `user_reviews`, `critic_reviews` and `popularity` are also numerical metrics that come from IMDB, and thus are already on their suitable form. By visual inspection, `awards_wins` and `awards_nominations` seem to count the number of awards and nominations other than the Oscars, but we will have to make sure of this. 

Then we have 16 binary columns saying whether a movie won or was nominated for one of the following Oscars' categories

1. Best Picture  
2. Best Director  
3. Best Actor  
4. Best Actress  
5. Best Supporting Actor  
6. Best Supporting Actress  
7. Best Adapted Screenplay
8. Best Original Screenplay

As we are concerned with predicting whether a movie will win at least one Oscar or not, we will create such column (to be called `Oscars_won_some`) using these and drop them after doing so. Note that we are only using data from 8 categories... (TODO: argue this..?). With regards to the nominated columns... TODO: We didn't discuss this, did we? Maybe we can use the next feature (see below)

Then we have another column called `Oscars_nominated`, which says for how many categories a movie was nominated for, including the ones not mentioned before. `Oscars_nominated_categories` is its analogous but on text form, indicating the specific nominated categories. We will also drop this one, TODO: right?. 

The rest of the features all follow the same pattern: first we have `award_name_won`, which says how many categories of such award the movie won, and then we have `award_name_won_categories`, a string specifying these categories. `award_name_nominated` and `award_name_nominated_categories` are the same but for nominations instead of wins. We will only use the numerical features, and drop the categorical ones (TODO: mention the other approach too).

## Missing Values

Having looked and experimented with several ways of imputing the missings we have decided to handle imputing on only the set of variables we expect to apply to the model for simplicity. See section [[Imputation]] below.


TODO: Talk about this...

## Quality Checks

Now let's make some quality checks, before diving into preprocessing. Our movies range from
```{r}
sort(unique(oscars$year))
```
and for each year, we have 
```{r}
table(oscars$year)
```
about 70 movies, but for 2017 we only have 30. We are thus not going to use the data from 2017, instead focusing on 2000-2016 data, which appears more complete.
```{r}
# Lied about not doing yet any preprocessing...
oscars = filter(oscars, year != 2017)
```

One other thing we must make sure of is that we have only 16 winners for each category. TODO: Do this...
```{r}
# Pattern to get only the relevant columns
pat.oscar_won <- "Oscar_.*_won$"
pat.oscar_nom <- "Oscar_.*_nominated$"

t(oscars %>% 
  select(matches(pat.oscar_won)) %>% 
  mutate_at(vars(matches(pat.oscar_won)), 
            funs(ifelse(. == "No", 0, 1))) %>%
  summarise_all(sum))
```

OK, so for now the data makes sense. However, it is very clear that our data is heavily imbalanced. TODO: Explain how we will deal with this later... examples: balancing the weights, oversampling? or maybe we don't need to.

Let's also make sure that `awards_wins` does not contain any information regarding the Oscars, as this would not be known for us when making future predictions. 

```{r}
# Gets the movies that won Oscars for best picture 
oscars_won_best_picture <- subset(oscars, Oscar_Best_Picture_won == "Yes")
dim(oscars_won_best_picture)
# Gets the first movie of such list
lord_of_the_rings_king <- head(oscars_won_best_picture, 1)
# Gets the number of awards it won, not counting the Oscars 
num_awards_won <- 0
# For every column name in the Series
for (i in names(lord_of_the_rings_king)){
  # If "won"" is in the column name at the end
  if(grepl("won$", i) == TRUE){
    # And if "Oscar" is not in the column name
     if(grepl("Oscar", i) == FALSE){
         # Add to num_awards_won the value of such column
         num_awards_won = num_awards_won + lord_of_the_rings_king[c(i)][[1]]
      }
  }
}
# Prints the value of awards_wins
lord_of_the_rings_king[c("awards_wins")]
# Prints the number of awards it won, not counting the Oscars
num_awards_won
```
(TODO: maybe do this for more movies, just to be sure). Good! This feature is fair and probably very important. 

I THINK THIS IS ALL FOR THE QUIALITY CHECKS AND ALSO EDA

# Model Building
## Data Preprocessing

Let's proceed on an orderly manner. First, let's get the target column, as it will be useful for later preprocessing. 

```{r oscars_won_some}
# sum number of wins identified per row for cols
tmp.won <- rowSums(
  oscars %>%
    mutate_at(
      vars(matches(pat.oscar_won)),
      funs(ifelse(. == "No", 0, 1))
    ) %>% select(matches(pat.oscar_won))
  )

# set win count variable in oscars df
oscars <- oscars %>% 
  mutate(Oscars_win_count = tmp.won)

# Validation
oscars_won_best_picture <- filter(oscars, Oscar_Best_Picture_won == 'Yes')
# This movie won 3 oscars, let's see if it checks out
oscars_won_best_picture$Oscars_win_count[1] == 3
# This movie won 2 oscars, let's see if it checks out
oscars_won_best_picture$Oscars_win_count[2] == 2
# Yeap

# Encodes as factor
oscars <- oscars %>% mutate(
  Oscars_won_some = factor(ifelse(Oscars_win_count > 0, "Yes", "No"))
)
```


Now, let's encode `certificate`, which follows this [system](https://www.wikiwand.com/en/Motion_Picture_Association_of_America_film_rating_system) and this [one](https://www.wikiwand.com/en/TV_parental_guidelines_(US)#/TV-MA). That is, `certificate` is an ordinal variable and needs to be encoded as such (preserving the order).
```{r}
# Prints the unique values of the certificate column
unique(oscars$certificate)
# Prints the movies "Unrated" and "Not Rated"
subset(oscars, certificate == "Not Rated")[c("movie")]
subset(oscars, certificate == "Unrated")[c("movie")]
```

Let's get rid of these rows. They would only make us create more columns for `certificate`, as we cannot encode them, and they are not well known movies (aside from The Great Beauty...) and did not win any Oscars.
```{r}
# Gets the indexes of such rows
not_rated_index = as.numeric(rownames(subset(oscars, certificate == "Not Rated")[c("movie")]))
unrated_index =  as.numeric(rownames(subset(oscars, certificate == "Unrated")[c("movie")]))

# Checks if any of them won an oscar
for(i in 1:nrow(oscars[not_rated_index, ])){
  if(oscars[not_rated_index, ]$Oscars_won_some[i] == "Yes"){
    print("Yes")
  }
}
for(i in 1:nrow(oscars[unrated_index, ])){
  if(oscars[unrated_index, ]$Oscars_won_some[i] == "Yes"){
    print("Yes")
  }
}
```

Assumption checked. Let us drop them.
```{r}
dim(oscars)
# https://github.com/tidyverse/dplyr/issues/3196
oscars = oscars %>% filter(certificate != "Not Rated" | is.na(certificate)) %>% filter(certificate != "Unrated" | is.na(certificate))
dim(oscars)
```

Now let us treat the missing values...
```{r}
unique(oscars$certificate)
sum(is.na(oscars$certificate))
```

well, let's actually see if we can get the missing values imputed first.
```{r}
titles_missing_values = oscars[is.na(oscars$certificate), ]$movie
titles_missing_values
```

If we look up these titles in IMDB, they are not rated. Let's check once again if they won any Oscars:
```{r}
for(i in titles_missing_values){
  if(subset(oscars, movie == i)$Oscars_won_some == "Yes"){
    print("Yes")}
}
```

The did not, so let's drop them too.
```{r}
oscars = oscars %>% filter(is.na(certificate) == FALSE)
dim(oscars)
```

And finally let's encode it...
```{r}
#TODO: I'm actually pretty sure this should just be a factor variable
unique(oscars$certificate)
oscars = oscars %>% mutate(certificate = replace(certificate, certificate == "PG-13", 3))
oscars = oscars %>% mutate(certificate = replace(certificate, certificate == "G", 1))
oscars = oscars %>% mutate(certificate = replace(certificate, certificate == "R", 4))
oscars = oscars %>% mutate(certificate = replace(certificate, certificate == "PG", 2))
oscars = oscars %>% mutate(certificate = replace(certificate, certificate == "TV-MA", 5))
oscars$certificate = as.numeric(oscars$certificate)
unique(oscars$certificate)
```


Moving on to `genre`, now we do need to create dummy variables
```{r}
# get all of the values parsed by |'s
genre_unique = unique(oscars$genre)
# get the actual unique values
soup = c()
for(i in 1:length(genre_unique)){
  soup = append(soup,unlist(strsplit(genre_unique[i],"\\|")))
}
new_cols = unique(soup)

# create a dataframe where the column names are the unique genres
gen = data.frame(matrix(nrow=nrow(oscars),ncol=length(new_cols)))
colnames(gen) = new_cols

for(i in 1:ncol(gen)){
  # iterate over columns
  for(j in 1:nrow(gen)){
    # then rows
    # if the string with the column name is in the string for the awards_won column in the original 
    # dataset.... give that variable a 1 in the new dataset 
    if((grepl(colnames(gen[i]),oscars$genre[j])==TRUE)){ 
      gen[j,i] = 1
      }
    else{
      gen[j,i] = 0
    }
  }
}

# we add the prefix "genre", so that the varaibles are easier to identify. 
colnames(gen) = paste("genre",colnames(gen),sep="_")

# get rid of the mispelling of history... somehow some moves were classified as histor and history
# assume music and musical are the same genre. 
gen$genre_History = gen$genre_History + gen$genre_Histor
gen$genre_Musical = gen$genre_Musical + gen$genre_Music


# check for duplicates
max(gen$genre_History)
max(gen$genre_Musical)
# we have them in both, so remove them. 

for(i in 1:length(gen$genre_History)){
  if(gen$genre_History[i]>1){
    gen$genre_History[i] =1
  }
}

for(i in 1:length(gen$genre_Musical)){
  if(gen$genre_Musical[i]>1){
    gen$genre_Musical[i] =1
  }
}

# select the columns that are duplicated
DropCols = c("genre_Histor","genre_Music")
# remove them from the datframe. 

gen = gen[,!colnames(gen)%in%DropCols]

oscars = cbind(oscars,gen)

# Changing 0 to "No" and 1 to "Yes" and converting to factor
for(i in names(gen)){
  oscars[c(i)][[1]] = ifelse(oscars[c(i)][[1]] == 1, "Yes", "No")
  oscars[c(i)][[1]] = factor(oscars[c(i)][[1]])
}
```
When we tried to run our model, we got this error ("Prints out this error: Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : contrasts can be applied only to factors with 2 or more levels"), and it was because there is only one movie that falls into the "documentary" genre, and thus only one level for `genre_documentary`
```{r}
table(oscars$genre_Documentary)
```
The movie in question is 
```{r}
subset(oscars, genre_Documentary == "Yes")$movie
```
and [here](https://www.imdb.com/title/tt5278466/awards) are the it received or was nominated for. As it is not that relevant and very annoying (TODO: Find a better way if there is one), let's drop it
```{r}
oscars <- oscars %>% select(-genre_Documentary)
```

The next feature that needs some manipulation is `gross`. The value of the dollar changed from 2000 to 2016, so we'll need to adjust the values we have to be based on a standard. We'll use the value of the dollar in the year 2000 as a common unit. To convert between 2001 through 2016 dollar amounts, and the 2000 dollar amounts we'll need something to use as a conversion factor. We'll use the Consumer Price Index (CPI) data provided by the United States Bureau of Labor Statistics, to get that conversion factor. Their site (https://data.bls.gov/timeseries/CUUR0000SA0), provides CPI data by month output to an excel sheet. We've opted to include the Annual average, and base our conversion on that. After cleaning the data up in excel, we saved it off in CPI_20181201.csv, which we'll load now.

```{r}
# Pull in Consumer Price Index Data from 
# Burea of Labor Statistics to account for inflation
cpi <- read_csv('CPI_20181201.csv', col_types = cols())
summary(cpi)
```

Now we just need a function for retrieving the correct CPI for a given year.

```{r}
# data ranged from 2000 (1) to 2017 (18)
# so... year mod 2000 + 1 is the indexing scheme.
# NOTE: if cpi csv file is changed the indexing scheme will need updating
cpif <- function(year) {
  idx <- year %% 2000 + 1
  cpi$Annual[idx]
}
```

Now we simply apply the conversion factor derived from the CPI data. 

```{r}

oscars = oscars %>% mutate(
  # Adjust gross field by Consumer Price Index.
  # cpif provides annual average CPI for specified year
  # data provided by the Burea of Labor Statistics website
  # (implementation included in full Rmd document)
  sc.gross = gross * cpif(2000)/cpif(year) # "In 2000 dollars"
) %>% 
  # TODO: Should this be here? thinking to move it down
  # mutate oscars won and oscars nominated cols/variables to factor types
  mutate_at(vars(matches(pat.oscar_won)), funs(factor)) %>%
  mutate_at(vars(matches(pat.oscar_nom)), funs(factor))
```

```{r currency_validation}
# Validation of currency adjustment
currency_info <- oscars %>% select(
  gross, sc.gross, year
) %>% group_by(year) %>% summarize(
  gross_mn = mean(gross, na.rm = TRUE),
  sc.gross_mn = mean(sc.gross, na.rm = TRUE)
)

currency_info %>% 
  gather(key, value, gross_mn, sc.gross_mn) %>%
  ggplot(aes(x=year, y=value, colour=key)) + 
  geom_line()
```

We also want to create a `season` variable, based on the release date information, and using the meteorlogically defined seasons in the northern hemisphere as described here: https://www.timeanddate.com/calendar/aboutseasons.html

```{r}
season <- function(month) {
  retVal <- ""
  
  if (month <= 2) {
    "Winter" # Winter [December, February]
  } else if (month <= 5) {
    retVal <- "Spring"  # Spring [March, May]
  } else if (month <= 8) {
    retVal <- "Summer" # Summer [June, August]
  } else if (month <= 11) {
    retVal <- "Fall" # Fall   [September, November]
  } else {
    retVal <- "Winter"
  }
  
  return (retVal)
}

oscars <- oscars %>%
  rowwise() %>%
  mutate(seasons = season(release_date.month))
```

All that is left is dropping the undesired columns, and ensuring the correct data types are being used.

```{r}
drop.cols <- c(
  'movie',
  'movie_id',
  'synopsis',
  'gross', # dropping gross b/c gross.sc contains scaled values
  'Oscars_win_count',
  'release_date',
  'Oscar_nominated_categories',
  'genre',
  'release_date.year',
  'release_date.day-of-month',
  'release_date.day-of-week',
  'release_date.month')
oscars <- oscars %>% 
  select(-one_of(drop.cols)) %>%
  select(-matches("categories$")) %>% 
  select(-matches("Oscar_.*_won$"))
```

## Imputation

Let's figure out to handle any missing values, now that we're removed some of the columns based on the EDA from above.

```{r}
missing_per_column = sapply(oscars, function(x) sum(is.na(x)))
missing_per_column[missing_per_column != 0]
```

Using the `tidyimpute` package to impute the missing values identified above based on the mean. After some consideration, this seemed the best approach to avoid having the imputation inject error into our model. 

```{r}
oscars <- oscars %>% impute_mean(
  metascore, 
  user_reviews, 
  critic_reviews, 
  popularity, 
  sc.gross
)
```

## Train and Test Split

and finally let's make sure that we only have the features we want for prediction, and that their types are the right ones (we did not drop `year` yet as we will use it for splitting intro training and testing)
```{r}
sapply(oscars, typeof)
```
TODO: again.. maybe we should drop the `Oscar_..._nominated` if we are only going to predict `Oscars_won_some` and use only `Oscar_nominated` instead. 

Everything seems fine. Let's move onto splitting the data. Let's use from 2000 to 2012 (~75%) for training and from 2013 to 2016 for testing. There is no need to stratify as we are not doing a random split, that is, both train and test sets will have the same proportion of class labels, because we are splitting by years. 
```{r}
# Splits by years
oscars_train <- subset(oscars, year %in% c(2000:2012)) 
oscars_test <- subset(oscars, year %in% c(2013:2016))

# Drops year
oscars <- select(oscars, -year)
oscars_train <- select(oscars_train, -year)
oscars_test <- select(oscars_test, -year)

# Gets the target
y_train = oscars_train$Oscars_won_some
y_test = oscars_test$Oscars_won_some
# Drops the target column
# TODO: Not sure it's necessary to drop target col?
X_train <- select(oscars_train, -Oscars_won_some)
X_test <- select(oscars_test, -Oscars_won_some)
```

The last step before we can train our models is standardizing. By default, R omits standardizing factor variables (TODO: Is this true?), so we do not need to be concerned about that. We will follow [this method](https://machinelearningmastery.com/pre-process-your-dataset-in-r/)
```{r}
scaleParam <- preProcess(oscars_train, method=c("center", "scale"))
oscars_train <- predict(scaleParam, oscars_train)
oscars_test <- predict(scaleParam, oscars_test)

scaleParam <- preProcess(X_train, method=c("center", "scale"))
X_train <- predict(scaleParam, X_train)
X_test <- predict(scaleParam, X_test)
```

Well... it seems we are ready to train some Machine Learning models!

# Modeling

## Logistic Regression

### Regular Logistic Regression

Let's just apply a Logistic Regression model based on all the available variables. 

```{r}
model.glm.all <- glm(Oscars_won_some~., family = binomial(link = "logit"), data = oscars_train)
```

In viewing the summary of this model we can see that something has clearly gone wrong, in fact R gives us a warning stating that the algorithm did not converge, and that fitted probabilities numerically 0 or 1 occurred. The standard errors are all in the thousands, the z value statistics are all close to zero, and the p-values are close to 1. accross the majority of the the variable types. There are also two independent variables whose coefficients that couldn't be estimated.

After a good deal of discussion, and experimentation we determined that the issue with this model was ultimately us having too few degrees of freedom for the model to successfully train with this many independent variables (hence why some estimates were NAs and p-values where all close to 1)  

Having done that, lets experiment with just using a subset of the variables available:

```{r}
# Trains the model
model.glm.subset <-  glm(Oscars_won_some~certificate + duration+rate+metascore+votes+user_reviews+critic_reviews+popularity+awards_wins+awards_nominations+sc.gross+seasons,
                family = binomial(link = "logit"), data = oscars_train)
```

The subsetted model shows that the model estimated all the coefficients, and has more reasonable z-value and p-value statistics. The most significant features by far, appear to be `certificate`, `user_reviews`, and `award_wins`.

Let's how well our model predicts the oscar winners in our training set. 

```{r}

# Tests the model
model.glm.subset.test <- predict.glm(model.glm.subset,oscars_test,type='response')

cm <- confusionMatrix(factor(ifelse(model.glm.subset.test > 0.5,"Yes","No")), y_test)

cm$table
cm$overall[1]
cm$overall[5]
```

The models hit rate is ~94%. But we need to consider that most of the movies in our dataset will not win an Oscar (~92%, as shown by AccuracyNull above). Our hit rate is higher than the percentage of movies that did not win Oscars, implying that our model's predictive capability, using a 50% threshold, is an improvement over simply using our target's distribution to predict the winners.

Although we had a high True Negative rate (~97%), our True positive rate was lower (~62%). This doesn't show in our hit rate, because only ~8% of our movies actually won oscars. Let us take a look at the ROC Curves, to see if a different threshold quantity would have a significant impact on the predictive capability of the model. 

```{r}
model.glm.subset.prob=plogis(predict.glm(model.glm.subset, type = c("response")))
#head(prob)
model.glm.subset.h <- roc(Oscars_won_some~model.glm.subset.prob, data=oscars_train)
plot(model.glm.subset.h)
# Area Under the Curve
auc(model.glm.subset.h)
```

The area under the curve is ~95%, which is greater than the no data rate of ~92% that we found before implying that our model discriminates well at different thresholds.

### Stepwise Logistic Regression

Now we'll attempt to build a Logistic Regrssion model doing feature selection using stepwise regression with BIC as an evaluation metric.

```{r}
# creating a null model
model.glm.null <-  glm(Oscars_won_some~1,
                family = binomial(link = "logit"), data = oscars_train)
# create a full model
model.glm.all <- glm(Oscars_won_some~.,
                family = binomial(link = "logit"), data = oscars_train)
```

```{r include=FALSE}
# use step to apply stepwise forward selection with BIC as a evaluation metric
model.glm.final <- step(model.glm.null, scope = formula(model.glm.all), direction = "forward", k = log(nrow(oscars_train)), trace = FALSE)
# NOTE: The output says AIC, but we really are calculating BIC, because we are using the log of the number of observations in our dataset as our penalty term (k) instead of just the number of predictors. This seemed appropriate due to the large number of features relative the number of observations available to us, and BIC penalizes extra features more than AIC does.
```

```{r}
summary(model.glm.final)
```

The Summary shows Std. Error terms that are much more reasonable, and only includes variables that are significant at least to the 5% level or higher. The forward stepwise variable selection chose 7 out of the 81 variables that were available to it, which allows for a much more interpretable model.

Let us see how well it predicts.

```{r include=FALSE}
model.glm.final.test <- predict.glm(model.glm.final,oscars_test,type='response')
```

```{r}
cm <- confusionMatrix(factor(ifelse(model.glm.final.test > 0.5,"Yes","No")),y_test)

cm$table
cm$overall[1]
cm$overall[5]
```

We can see that accuracy (~97%) is appoximately 3% better than the accuracy of our manually subsetted logistic regression model from above. We aso see from the confusion table, that our True Negative rate for this model is (~98%), which is ~1% higher than the subsetted model above. Our True Positive rate in this model is ~81%, showing a dramatic increase of ~19 percentage points over the subsetted model above. 



The average accuracy from the cross-validation approach (96.89%) is comparable to the accuracy shown in our model trained only from a single training set (96.93%). The standard deviation of the cross-validation accuracy is also quite low, suggesting that our model is not overfitting our data. 

We still want to see how well our model discriminates at different thresholds:

```{r}
# ROC Curve
prob=plogis(predict.glm(model.glm.final, type = c("response")))
#head(prob)
h <- roc(Oscars_won_some~prob, data=oscars_train)
plot(h)
# Area Under the Curve
auc(h)
```

The ROC plot shows a definite improvement relative to the previous model, and the AUC is increased by ~3 percentage points (total of ~98%). 

We also need to confirm that we aren't seeing a massive amount of multicollinearity:

```{r}
vif(model.glm.final)
```

Due to the large values for `award_wins` and `Online_Film_Television_Association_nominated`, we should probably repeat the stepwise regression process using a subset of the variables that aren't the variables that are aggregated into `award_wins`, while keeping `award_wins` available for use by the stepwise variable selection process.

```{r}
oscars_train2 <- oscars_train %>% select(-matches(".*nominated$"), -matches(".*won$"))
oscars_test2 <- oscars_test %>% select(-matches(".*nominated$"), -matches(".*won$"))

# creating a null model
model.glm.null2 <-  glm(Oscars_won_some~1,
                family = binomial(link = "logit"), data = oscars_train2)
# create a full model
model.glm.all2 <- glm(Oscars_won_some~.,
                family = binomial(link = "logit"), data = oscars_train2)
```

```{r echo=TRUE}
# use step to apply stepwise forward selection with BIC as a evaluation metric
model.glm.final2 <- step(model.glm.null2, scope = formula(model.glm.all2), direction = "forward", k = log(nrow(oscars_train)), trace = FALSE)
# NOTE: The output says AIC, but we really are calculating BIC, because we are using the log of the number of observations in our dataset as our penalty term (k) instead of just the number of predictors. This seemed appropriate due to the large number of features relative the number of observations available to us, and BIC penalizes extra features more than AIC does.
```

```{r}
summary(model.glm.final2)
```

```{r}
model.glm.final2.test <- predict.glm(model.glm.final2,oscars_test2,type='response')
cm <- confusionMatrix(factor(ifelse(model.glm.final2.test > 0.5,"Yes","No")),y_test)

cm$table
cm$overall[1]
cm$overall[5]
```

```{r}
vif(model.glm.final2)
```

Let's use 3-fold stratified cross-validation to test the variance of our predictive accuracy. We chose stratified cross-validation to ensure the proportion of class labels is maintained in each of the splits. We chose 3-folds instead of the standard 10-folds, because if we were to use smaller splits we'd risk having too few degrees of freedom to train the model, given the scarcity of the oscar winners.

```{r}
names(model.glm.final$coefficients)
# Generates a list of indexes for each stratified split
folds <- KFold(oscars$Oscars_won_some, n = 3, stratified = TRUE, seed = 42)
# Gets a list with the number of folds
list_ = 1:length(folds)
# Gets a list to store the accuracy for each split
scores = c()
# For each fold
for(i in list_){
  # Gets the indexes for getting the training data
  list_train = list_[-i]
  train_index = c(folds[[list_train[1]]], folds[[list_train[2]]])
  # Gets the index for the testing data
  test_index = folds[[i]]
  # Splits between training and testing
  cv.train = oscars[train_index, ]
  cv.test = oscars[test_index, ]
  # Trains the model 
  logreg.train <- glm(Oscars_won_some~awards_wins +
        Screen_Actors_Guild_won+Golden_Globes_won+genre_Drama
      +Writers_Guild_won+Online_Film_Television_Association_nominated
      +BAFTA_won, family = binomial(link = "logit"),
      data = cv.train)
  # Tests the model
  glm.final.test <- predict.glm(logreg.train,cv.test,type='response')
  score <- sum(ifelse(glm.final.test > 0.5,"Yes","No")
             ==cv.test$Oscars_won_some)/
  length(cv.test$Oscars_won_some)
  # Appends the score to the list 
  scores = c(scores, score)
}
scores
mean(scores)
sd(scores)
```


### Approaches

## Random Forest

### Su

### b

### Sections



# Conclusions 

