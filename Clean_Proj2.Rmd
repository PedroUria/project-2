---
title: "Predicting the Oscars"
date: "12/14/2018"
output:
  html_document:
    code_folding: hide
    highlight: tango
    number_sections: yes
    theme: united
    toc: yes
  pdf_document:
    toc: yes
authors: Pedro Uria, Sean Pili and Zachary Buckley
---

```{r setup, include=FALSE}
# !diagnostics off
knitr::opts_chunk$set(echo = TRUE)
```

# Stating the Question

Can we predict whether a movie will win at least one Oscar?

## Objective

The goal of this project is to develop models to predict whether a movie will win some kind of Oscar award or not. In order to achieve this, we will use a dataset that [BigMl](https://bigml.com/) [put together](https://bigml.com/user/academy_awards/gallery/dataset/5a94302592fb565ed400103b) and used to train a deep neural network to get [perfect predictions](https://blog.bigml.com/2018/03/01/predicting-the-2018-oscar-winners/) for the 8 categories they targeted for 2018. However, this does not mean their model is perfect, as ..... (TODO: source for where they say even they were surprised?).

## Motivation

The main motivation is fun and also to have an edge when betting for the next Oscars winners. TODO: And to use these models to identify the most relevant features that make a movie win an Academy Award, and also 

## The Data

Gathering the data was very straightforward in this case. After coming up with the idea for the project, a simple Google search of: "*oscars machine learning*" almost directly handed us the dataset. We quickly found [this article](https://medium.com/enrique-dans/and-this-years-oscar-goes-to-bigml-machine-learning-1823837ae3aa), which lead us to BigML. After signing up on their website, we were able to download the dataset for free. However, this would not mean that the data would be ready to be fed to our machine learning algorithms right away... which we quickly realized after doing some EDA. BigML combined data from IMDB with entries that specify whether a movie has won or has been nominated for some other awards previous to the Oscars.

# Exploratory Data Analysis (EDA)

```{r install_functions, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
##Helper Functions here for cleanly handling imports
#install_all - function for installing all packages in a list that aren't available
#  inspired by https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them
#  ex: install_all(c('readr', 'dplyr'))
install_all <- function(list.of.packages) {
  need_install <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
  if(length(need_install)) install.packages(need_install)
}

# package_apply_all - function for calling install_all on list of packages, then applying a function to them
#  inspired by https://stackoverflow.com/questions/8175912/load-multiple-packages-at-once
#  ex: package_apply_all(c('readr', 'dplyr'), require)
package_apply_all <- function(list.of.packages, func) {
  install_all(list.of.packages)
  invisible(lapply(list.of.packages, func, character.only = TRUE))
}
```

```{r imports, include=FALSE}
package_apply_all(
  c(
    'readr', # for loading data from various formats
    'ggplot2', # pretty graphs
    'tidyr', # needed for the gather function
    'caret', # needed for standardizing
    'pROC', # needed for ROC curve
    'dplyr', # because we all need a little magic
    'import',
    'rBayesianOptimization', # used in cross-validation of step-wise logistic
    'car',
    'randomForest'
  ), library)
install_all(c(
  'tidyimpute',
  'caret'
))
import::from(tidyimpute, "impute_mean")
import::from(caret, "confusionMatrix")

```

## First Contact

Let's load the data and see what features we have and how large our dataset is
```{r}
oscars <- read_csv("oscars.csv", col_types = cols())
#View(head(oscars)) # Go to the .Rdm file and uncomment this line
# if you want to have a look at the data
dim(oscars)
#sapply(oscars, typeof) # Same here
```

So we have 1183 rows and 119 features. `year` will be useful to make a train-test split later, but we will drop it after that. `movie` and `movie_id` will also be dropped as they are not relevant for prediction. We will need to encode `certificate`, while `duration` is already in a suitable form, and `genre` will have to be converted to dummy variables. `rate` and `metascore` are also fine. We will also drop `synopsis`, which is very unlikely to matter significantly, and which would also require Natural Language Processing, as skill we lack? (TODO: maybe delete the NLP part). `votes` is also fine, while `gross` will need to be modified to account for inflation over the years. We will also drop `release_date` but will use `release_date.month` (at the end of the features) to create a new column `season`, which is known to be a relevant factor in winning an Oscar. `release_date.year`, `release_date.day-of-month` and `release_date.day-of-week` will also be dropped. `user_reviews`, `critic_reviews` and `popularity` are also numerical metrics that come from IMDB, and thus are already on their suitable form. By visual inspection, `awards_wins` and `awards_nominations` seem to count the number of awards and nominations other than the Oscars, but we will have to make sure of this. 

Then we have 16 binary columns saying whether a movie won or was nominated for one of the following Oscars' categories:

1. Best Picture  
2. Best Director  
3. Best Actor  
4. Best Actress  
5. Best Supporting Actor  
6. Best Supporting Actress  
7. Best Adapted Screenplay
8. Best Original Screenplay

As we are concerned with predicting whether a movie will win at least one Oscar or not, we will create such column (to be called `Oscars_won_some`) using these, and drop them after doing so. Note that we are only using data from 8 categories, which is what BigML had. Then we have another column called `Oscars_nominated`, which says for how many categories a movie was nominated for, including the ones not mentioned before. `Oscars_nominated_categories` is its analogous but on text form, indicating the specific nominated categories. We will also drop this one.

The rest of the features all follow the same pattern: first we have `award_name_won`, which says how many categories of such award the movie won, and then we have `award_name_won_categories`, a string specifying these categories. `award_name_nominated` and `award_name_nominated_categories` are the same but for nominations instead of wins. We will only use the numerical features, and drop the categorical ones.

## Missing Values

Having looked and experimented with several ways of imputing the missings we have decided to handle imputing on only the set of variables we expect to apply to the model for simplicity. See section [[Imputation]] below.

While normally the missingness is the first thing to be dealt with, in our case it makes more sense to deal with it after getting rid of all the features we do not want to use.

## Quality Checks

Now let us make some quality checks, before diving into preprocessing. Our movies range from
```{r}
sort(unique(oscars$year))
```
and for each year, we have 
```{r}
table(oscars$year)
```
about 70 movies, but for 2017 we only have 30. We are thus not going to use the data from 2017, and will instead focus on 2000-2016 data, which is clearly more complete.
```{r}
# Lied about not doing yet any preprocessing...
oscars = filter(oscars, year != 2017)
```

One other thing we must make sure of is that we have only 17 winners for each category. 
```{r}
# Pattern to get only the relevant columns
pat.oscar_won <- "Oscar_.*_won$"
pat.oscar_nom <- "Oscar_.*_nominated$"

t(oscars %>% 
  select(matches(pat.oscar_won)) %>% 
  mutate_at(vars(matches(pat.oscar_won)), 
            funs(ifelse(. == "No", 0, 1))) %>%
  summarise_all(sum))
```

OK, so for now the data makes sense. However, it is very clear that our data is heavily imbalanced. That is, at best only 8 movies will have a `Yes` entry on the `Oscars_won_some` column (which we have not computed yet) for each year. At the end of the project will try to deal with this by applying sampling methods, and compare the results.

Let's also make sure that `awards_wins` does not contain any information regarding the Oscars, as this would not be known to us when making future predictions
```{r echo=TRUE}
# Gets the movies that won Oscars for best picture 
oscars_won_best_picture <- subset(oscars, Oscar_Best_Picture_won == "Yes")
dim(oscars_won_best_picture)
# Gets the first movie of such list
lord_of_the_rings_king <- head(oscars_won_best_picture, 1)
# Gets the number of awards it won, not counting the Oscars 
num_awards_won <- 0
# For every column name in the Series
for (i in names(lord_of_the_rings_king)){
  # If "won"" is in the column name at the end
  if(grepl("won$", i) == TRUE){
    # And if "Oscar" is not in the column name
     if(grepl("Oscar", i) == FALSE){
         # Add to num_awards_won the value of such column
         num_awards_won = num_awards_won + lord_of_the_rings_king[c(i)][[1]]
      }
  }
}
# Prints the value of awards_wins
lord_of_the_rings_king[c("awards_wins")]
# Prints the number of awards it won, not counting the Oscars
num_awards_won
```
(TODO: maybe do this for more movies, just to be sure). Good! This feature is fair and probably very important. Maybe some dplyr magic... 

# Data Preprocessing

## Feature Manipulation

Let us proceed on an orderly manner. First, we will compute the target column, as it will be useful for later preprocessing. 
```{r oscars_won_some}
# sum number of wins identified per row for cols
tmp.won <- rowSums(
  oscars %>%
    mutate_at(
      vars(matches(pat.oscar_won)),
      funs(ifelse(. == "No", 0, 1))
    ) %>% select(matches(pat.oscar_won))
  )

# set win count variable in oscars df
oscars <- oscars %>% 
  mutate(Oscars_win_count = tmp.won)

# Validation
oscars_won_best_picture <- filter(oscars, Oscar_Best_Picture_won == 'Yes')
# This movie won 3 oscars, let's see if it checks out
oscars_won_best_picture$Oscars_win_count[1] == 3
# This movie won 2 oscars, let's see if it checks out
oscars_won_best_picture$Oscars_win_count[2] == 2
# Yeap

# Encodes as factor
oscars <- oscars %>% mutate(
  Oscars_won_some = factor(ifelse(Oscars_win_count > 0, "Yes", "No"))
)
```

Now, let us encode `certificate`, which follows this [system](https://www.wikiwand.com/en/Motion_Picture_Association_of_America_film_rating_system) and this [one](https://www.wikiwand.com/en/TV_parental_guidelines_(US)#/TV-MA). That is, `certificate` is an ordinal variable and needs to be encoded as such (preserving the order). Well... actually, here we had a very interesting discussion: even if there is some order in `certifiacte`, we would not know how to interpret this order, and it could also be argued that there is no order to begin with. Thus, we tried these tro approaches:

1. Turn certificate into a `factor`, which will then be one-hot-encoded by R when running Logistic Regression.
2. Encode certificate presersving the order.

It turned out that option 2. worked best, even when the order was not clear, it seemed it did have a say in the pattern our data followed. The difference was not really that great, but even so, small improvements on an accuracy range of 92% to 100% do matter. Below we proceed with approach 2.

```{r}
# Prints the unique values of the certificate column
unique(oscars$certificate)
# Prints the movies "Unrated" and "Not Rated"
subset(oscars, certificate == "Not Rated")[c("movie")]
subset(oscars, certificate == "Unrated")[c("movie")]
```

Let us get rid of these rows. They would only make us create more columns for `certificate`, as we cannot encode them (which integer would we assign them?), and they are not well known movies (aside from *The Great Beauty*...), and did not win any Oscars.
```{r}
# Gets the indexes of such rows
not_rated_index = as.numeric(rownames(subset(oscars, certificate == "Not Rated")[c("movie")]))
unrated_index =  as.numeric(rownames(subset(oscars, certificate == "Unrated")[c("movie")]))

# Checks if any of them won an oscar
for(i in 1:nrow(oscars[not_rated_index, ])){
  if(oscars[not_rated_index, ]$Oscars_won_some[i] == "Yes"){
    print("Yes")
  }
}
for(i in 1:nrow(oscars[unrated_index, ])){
  if(oscars[unrated_index, ]$Oscars_won_some[i] == "Yes"){
    print("Yes")
  }
}
```

Assumption checked. Let us drop them.
```{r}
dim(oscars)
# https://github.com/tidyverse/dplyr/issues/3196
oscars = oscars %>% filter(certificate != "Not Rated" | is.na(certificate)) %>% filter(certificate != "Unrated" | is.na(certificate))
dim(oscars)
```

Now let us treat the missing values...
```{r}
unique(oscars$certificate)
sum(is.na(oscars$certificate))
```

well, we could actually try to impute the missing values first. 
```{r}
titles_missing_values = oscars[is.na(oscars$certificate), ]$movie
titles_missing_values
```

If we look up these titles in IMDB, they are not rated. Let's check once again if they won any Oscars:
```{r}
for(i in titles_missing_values){
  if(subset(oscars, movie == i)$Oscars_won_some == "Yes"){
    print("Yes")}
}
```

The did not, so let's drop them too.
```{r}
oscars = oscars %>% filter(is.na(certificate) == FALSE)
dim(oscars)
```

And finally let's encode it...
```{r}
unique(oscars$certificate)
oscars = oscars %>% mutate(certificate = replace(certificate, certificate == "PG-13", 3))
oscars = oscars %>% mutate(certificate = replace(certificate, certificate == "G", 1))
oscars = oscars %>% mutate(certificate = replace(certificate, certificate == "R", 4))
oscars = oscars %>% mutate(certificate = replace(certificate, certificate == "PG", 2))
oscars = oscars %>% mutate(certificate = replace(certificate, certificate == "TV-MA", 5))
oscars$certificate = as.numeric(oscars$certificate)
unique(oscars$certificate)
```


In case you want to try approach 1., in the `.Rmd` file there is a function designed to do this. We have decided to hide it in the `.html` file and set not to run by default, but you can go to the `.Rdm` file and run it (without runing all the code above concerning `certificate`), and then look at the differences in results. Just uncomment the last line of code of the hidden code chunk.
```{r eval=FALSE, include=FALSE}
genre_factor = function(){
  # Checking missing values
  unique(oscars$certificate)
  sum(is.na(oscars$certificate))
  # We could try to impute them by looking them up on the web
  titles_missing_values = oscars[is.na(oscars$certificate), ]$movie
  titles_missing_values
  # In this case we are keeping Not Rated and Unrated, so this will be a slightly different approach
  # If we look up these titles in IMDB, two of them do not have any entry specifying rating, and the others are "Not Rated". This means that we can turn the NAs to `Not Rated`, and we should also turn the `Unrated` to `Not Rated`, in order not to create unneccessary dummy features
  oscars = oscars %>% mutate(certificate = replace(certificate, certificate == "Unrated", "Not Rated"))
  oscars$certificate[is.na(oscars$certificate)] = "Not Rated"
  # When we tested our first model, we got this error: "Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : factor certificate has new levels TV-MA". This was because there is only one movie with `certificate` = `TV-MA`
  table(oscars$certificate)
  # This movie is
  subset(oscars, certificate == "TV-MA")$movie
  # and although it did win an Oscar and 5 other [awards](https://www.imdb.com/title/tt5278466/awards?ref_=tt_awd), our only option is to drop it.
  # Note that this movie is the same one that gave us problems on genre (se below), so if you run this piece of code, it will already be dropped on the genre step, so it will give an error because we will be attepting to drop a movie that is not there, but this is no reason for concern
  # Finally, let us turn `certificate` into a factor
  return(factor(oscars$certificate))
}

#oscars$certificate = genre_factor()
```

Moving on to `genre`, now we will manually create dummy variables, because we have movies with more than one genre, and simply turning it into a factor would result on too many lavels
```{r}
# get all of the values parsed by |'s
genre_unique = unique(oscars$genre)
# get the actual unique values
soup = c()
for(i in 1:length(genre_unique)){
  soup = append(soup,unlist(strsplit(genre_unique[i],"\\|")))
}
new_cols = unique(soup)

# create a dataframe where the column names are the unique genres
gen = data.frame(matrix(nrow=nrow(oscars),ncol=length(new_cols)))
colnames(gen) = new_cols

for(i in 1:ncol(gen)){
  # iterate over columns
  for(j in 1:nrow(gen)){
    # then rows
    # if the string with the column name is in the string for the awards_won column in the original 
    # dataset.... give that variable a 1 in the new dataset 
    if((grepl(colnames(gen[i]),oscars$genre[j])==TRUE)){ 
      gen[j,i] = 1
      }
    else{
      gen[j,i] = 0
    }
  }
}

# we add the prefix "genre", so that the varaibles are easier to identify. 
colnames(gen) = paste("genre",colnames(gen),sep="_")

# get rid of the mispelling of history... somehow some moves were classified as histor and history
# assume music and musical are the same genre. 
gen$genre_History = gen$genre_History + gen$genre_Histor
gen$genre_Musical = gen$genre_Musical + gen$genre_Music


# check for duplicates
max(gen$genre_History)
max(gen$genre_Musical)
# we have them in both, so remove them. 

for(i in 1:length(gen$genre_History)){
  if(gen$genre_History[i]>1){
    gen$genre_History[i] =1
  }
}

for(i in 1:length(gen$genre_Musical)){
  if(gen$genre_Musical[i]>1){
    gen$genre_Musical[i] =1
  }
}

# select the columns that are duplicated
DropCols = c("genre_Histor","genre_Music")
# remove them from the datframe. 

gen = gen[,!colnames(gen)%in%DropCols]

oscars = cbind(oscars,gen)

# Changing 0 to "No" and 1 to "Yes" and converting to factor
for(i in names(gen)){
  oscars[c(i)][[1]] = ifelse(oscars[c(i)][[1]] == 1, "Yes", "No")
  oscars[c(i)][[1]] = factor(oscars[c(i)][[1]])
}
```

When we tried to run our model, we got this error: "Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : contrasts can be applied only to factors with 2 or more levels", and it was because there is only one movie that falls into the "documentary" genre, and thus only one level for `genre_documentary`
```{r}
table(oscars$genre_Documentary)
```
The movie in question is 
```{r}
subset(oscars, genre_Documentary == "Yes")$movie
```
and [here](https://www.imdb.com/title/tt5278466/awards) are the it received or was nominated for. As it is not that relevant and very annoying, let's drop it
```{r}
oscars <- oscars %>% select(-genre_Documentary)
``` 

The next feature that needs some manipulation is `gross`. The value of the dollar changed from 2000 to 2016, so we'll need to adjust the values we have to be based on a standard. We'll use the value of the dollar in the year 2000 as a common unit. To convert between 2001 through 2016 dollar amounts, and the 2000 dollar amounts we'll need something to use as a conversion factor. We'll use the Consumer Price Index (CPI) data provided by the United States Bureau of Labor Statistics, to get that conversion factor. Their [site](https://data.bls.gov/timeseries/CUUR0000SA0), provides CPI data by month output to an excel sheet. We've opted to include the Annual average, and base our conversion on that. After cleaning the data up in excel, we saved it off in CPI_20181201.csv, which we'll load now
```{r}
# Pull in Consumer Price Index Data from 
# Burea of Labor Statistics to account for inflation
cpi <- read_csv('CPI_20181201.csv', col_types = cols())
summary(cpi)
```

Now we just need a function for retrieving the correct CPI for a given year
```{r}
# data ranged from 2000 (1) to 2017 (18)
# so... year mod 2000 + 1 is the indexing scheme.
# NOTE: if cpi csv file is changed the indexing scheme will need updating
cpif <- function(year) {
  idx <- year %% 2000 + 1
  cpi$Annual[idx]
}
```

Now we simply apply the conversion factor derived from the CPI data
```{r}

oscars = oscars %>% mutate(
  # Adjust gross field by Consumer Price Index.
  # cpif provides annual average CPI for specified year
  # data provided by the Burea of Labor Statistics website
  # (implementation included in full Rmd document)
  sc.gross = gross * cpif(2000)/cpif(year) # "In 2000 dollars"
) %>% 
  # TODO: Should this be here? thinking to move it down
  # Where would you like to put it? I think it's fine
  # mutate oscars won and oscars nominated cols/variables to factor types
  mutate_at(vars(matches(pat.oscar_won)), funs(factor)) %>%
  mutate_at(vars(matches(pat.oscar_nom)), funs(factor))
```

```{r currency_validation}
# Validation of currency adjustment
currency_info <- oscars %>% select(
  gross, sc.gross, year
) %>% group_by(year) %>% summarize(
  gross_mn = mean(gross, na.rm = TRUE),
  sc.gross_mn = mean(sc.gross, na.rm = TRUE)
)

currency_info %>% 
  gather(key, value, gross_mn, sc.gross_mn) %>%
  ggplot(aes(x=year, y=value, colour=key)) + 
  geom_line()
```


We also want to create a `season` variable, based on the release date information, and using the meteorologically defined seasons in the northern hemisphere as described [here](https://www.timeanddate.com/calendar/aboutseasons.html)
```{r}
season <- function(month) {
  retVal <- "Fall"
  
  if (month <= 2) {
    "Winter" # Winter [December, February]
  } else if (month <= 5) {
    retVal <- "Spring"  # Spring [March, May]
  } else if (month <= 8) {
    retVal <- "Summer" # Summer [June, August]
  } else if (month <= 11) {
    retVal <- "Fall" # Fall   [September, November]
  } else {
    retVal <- "Winter"
  }
  
  return (retVal)
}

oscars <- oscars %>%
  rowwise() %>%
  mutate(seasons = season(release_date.month))
oscars$seasons = factor(oscars$seasons)
```

All that is left is dropping the undesired columns
```{r}
drop.cols <- c(
  'movie',
  'movie_id',
  'synopsis',
  'gross', # dropping gross b/c gross.sc contains scaled values
  'Oscars_win_count',
  'release_date',
  'Oscar_nominated_categories',
  'genre',
  'release_date.year',
  'release_date.day-of-month',
  'release_date.day-of-week',
  'release_date.month')
oscars <- oscars %>% 
  select(-one_of(drop.cols)) %>%
  select(-matches("categories$")) %>% 
  select(-matches("Oscar_.*_won$"))
```

## Imputation

Let us figure out how to handle any missing values, now that we have removed some of the columns based on the EDA from above 
```{r}
missing_per_column = sapply(oscars, function(x) sum(is.na(x)))
missing_per_column[missing_per_column != 0]
```

Using the `tidyimpute` package, we will impute the missing values identified above based on the mean. After some consideration, this seemed the best approach to avoid having the imputation inject error into our model
```{r}
oscars <- oscars %>% impute_mean(
  metascore, 
  user_reviews, 
  critic_reviews, 
  popularity, 
  sc.gross
)
```

and finally let us make sure that we only have the features we want for prediction, and that their types are the right ones (we did not drop `year` yet as we will use it for splitting intro training and testing)
```{r}
#sapply(oscars, typeof) #Uncomment this line if you want to check the types
```

## Train and Test Split

Everything seems fine. Let us move onto splitting the data. We are going to use data from 2000 to 2012 ($\approx$75%) for training and from 2013 to 2016 for testing. There is no need to stratify as we are not doing a random split, that is, both train and test sets will have the same proportion of class labels, because we are splitting by years
```{r}
# Splits by years
oscars_train <- subset(oscars, year %in% c(2000:2012)) 
oscars_test <- subset(oscars, year %in% c(2013:2016))

# Drops year
oscars <- select(oscars, -year)
oscars_train <- select(oscars_train, -year)
oscars_test <- select(oscars_test, -year)

# Gets the target
y_train = oscars_train$Oscars_won_some
y_test = oscars_test$Oscars_won_some
```

## Standardization

The last step before we can train our models is standardizing. By default, R omits standardizing factor variables, so we do not need to be concerned about that. We will follow [this method](https://machinelearningmastery.com/pre-process-your-dataset-in-r/)
```{r}
scaleParam <- preProcess(oscars_train, method=c("center", "scale"))
oscars_train <- predict(scaleParam, oscars_train)
oscars_test <- predict(scaleParam, oscars_test)
```

Well... it seems we are ready to train some Machine Learning models!

# Model Building

We decided to tackle the prediction problem using Logistic Regression and Random Forest classifiers due to the categorical nature of the dependent variable. Apart from using our train and test sets to evaluate our models, we will also use 3-fold stratified cross-validation to test the variance of our predictive accuracy. We chose stratified cross-validation to ensure the proportion of class labels is maintained in each of the splits. We chose 3-folds instead of the standard 10-folds, because if we were to use smaller splits we would risk having too few testing data, and thus our metrics would not be accurate enough.

## Logistic Regression

First we will build a Multiple Logistic Regression model, then we shall attempt to improve upon it using step-wise forward variable selection. 

### Regular Logistic Regression

First we will apply a Logistic Regression model based on all the available variables. 

```{r}
model.glm.all <- glm(Oscars_won_some~., family = binomial(link = "logit"), data = oscars_train)
```

In viewing the summary of this model we can see that something has clearly gone wrong, in fact R gives us a warning stating that the algorithm did not converge, and that fitted probabilities numerically 0 or 1 occurred. The standard errors are all in the thousands, the z value statistics are all close to zero, and the p-values are close to 1 across the majority of the the variable types. There are also two independent variables whose coefficients could not be estimated.

After a good deal of discussion and experimentation, we determined that the issue with this model was ultimately us having too few degrees of freedom for the model to successfully train with these many independent variables (hence why some estimates were NAs and p-values where all close to 1)  

Having done that, lets experiment with just using a subset of the variables available
```{r}
# Trains the model
model.glm.subset <-  glm(Oscars_won_some~certificate + duration+rate+metascore+votes+user_reviews+critic_reviews+popularity+awards_wins+awards_nominations+sc.gross+seasons,
                family = binomial(link = "logit"), data = oscars_train)
```

The subsetted model shows that the model estimated all the coefficients, and has more reasonable z-value and p-value statistics. The most significant features by far, appear to be `certificate`, `user_reviews`, and `award_wins`.

Well, let us test this very simple model using our testing set
```{r}
# Tests the model
model.glm.subset.test <- predict.glm(model.glm.subset,oscars_test,type='response')

cm <- confusionMatrix(factor(ifelse(model.glm.subset.test > 0.5,"Yes","No")), y_test)

cm$table
cm$overall[1]
cm$overall[5]
```

The models hit rate is ~94%. But we need to consider that most of the movies in our dataset will not win an Oscar (~92%, as shown by AccuracyNull above). Our hit rate is higher than the percentage of movies that did not win Oscars, implying that our model's predictive capability, using a 50% threshold, is an improvement over simply using our target's distribution to predict the winners (this means classifying all movies as not winners).

Although we had a high True Negative rate (~97%), our True positive rate was lower (~62%). This does not show in our hit rate, because only ~8% of our movies actually won Oscars. Let us take a look at the ROC Curves, to see if a different threshold quantity would have a significant impact on the predictive capability of the model
```{r}
model.glm.subset.prob=plogis(predict.glm(model.glm.subset, type = c("response")))
#head(prob)
model.glm.subset.h <- roc(Oscars_won_some~model.glm.subset.prob, data=oscars_train)
plot(model.glm.subset.h)
# Area Under the Curve
auc(model.glm.subset.h)
```
The area under the curve is ~95%, which is greater than the no data rate of ~92% that we found before implying that our model discriminates well at different thresholds.


Before moving on to step-wise forward selection, we shall verify our results using cross-validation
```{r}
# cross_validate function for looping over multiple stratified splits, training a model, and testing. 
cross_validate <- function(data, fTrain, fHitRate) {
  folds <- KFold(data$Oscars_won_some, n = 3, 
                 stratified = TRUE, seed = 42)
  # Gets a list with the number of folds
  list_ = 1:length(folds)
  # Gets a list to store the accuracy for each split
  scores = c()
  # For each fold
  for(i in list_){
    # Gets the indexes for getting the training data
    list_train = list_[-i]
    train_index = c(folds[[list_train[1]]], folds[[list_train[2]]])
    # Gets the index for the testing data
    test_index = folds[[i]]
    # Splits between training and testing
    cv.train = data[train_index, ]
    cv.test = data[test_index, ]
    # Standardizes the new data
    scaleParam <- preProcess(cv.train, method=c("center", "scale"))
    cv.train <- predict(scaleParam, cv.train)
    cv.test <- predict(scaleParam, cv.test)
    # Trains the model 
    trained.model <- fTrain(cv.train)
    # Tests the model
    y.predicted <-
      predict(
        trained.model,
        cv.test,
        type='response')
    
    score <- fHitRate( y.predicted, cv.test$Oscars_won_some)
    # Appends the score to the list 
    scores = c(scores, score)
  }
  return(scores)
}

# Define function for training the model in the cross-validation loop
model.glm.subset.fTrain <- function(train_data) {
     return(
       glm(Oscars_won_some~certificate +
             duration +
             rate +
             metascore +
             votes +
             user_reviews +
             critic_reviews +
             popularity +
             awards_wins +
             awards_nominations +
             sc.gross +
             seasons,
           family = binomial(link = "logit"),
           data = train_data)
     )
}

# define the function for computing the hit rate in the cross-validation loop
model.glm.fHitRate <- function(pred_y, orig_y) {
  a <- sum( ifelse(pred_y > 0.5,"Yes","No") == orig_y)
  b <- length(orig_y)
  return (a / b)
}
  
# run the cross validation function, and review results
scores <- oscars %>% cross_validate(
  fTrain = model.glm.subset.fTrain,
  fHitRate = model.glm.fHitRate)

scores
mean(scores)
sd(scores)
```
Upon the looks of this, our model has a relatively high variance when using trained and test on different sets. This means that this model will likely not generalize well on new unseen data, and thus will be unreliable for future predictions. 

### Stepwise Logistic Regression

Now we will attempt to build a Logistic Regression model doing feature selection using stepwise regression with BIC as an evaluation metric.

```{r}
# creating a null model
model.glm.null <-  glm(Oscars_won_some~1,
                family = binomial(link = "logit"), data = oscars_train)
# create a full model
model.glm.all <- glm(Oscars_won_some~.,
                family = binomial(link = "logit"), data = oscars_train)
```
Note that these warnings ... TODO: Explain why we should not worry about them

```{r include=FALSE}
# use step to apply stepwise forward selection with BIC as a evaluation metric
model.glm.final <- step(model.glm.null, scope = formula(model.glm.all), direction = "forward", k = log(nrow(oscars_train)), trace = FALSE)
# NOTE: The output says AIC, but we really are calculating BIC, because we are using the log of the number of observations in our dataset as our penalty term (k) instead of just the number of predictors. This seemed appropriate due to the large number of features relative the number of observations available to us, and BIC penalizes extra features more than AIC does.
```

```{r}
summary(model.glm.final)
```

The Summary shows Std. Error terms that are much more reasonable, and only includes variables that are significant at least to the 5% level or higher. The forward stepwise variable selection chose 7 out of the 81 variables that were available to it, which allows for a much more interpret able model.

Let us see how well it predicts
```{r include=FALSE}
model.glm.final.test <- predict.glm(model.glm.final,oscars_test,type='response')
```

```{r}
cm <- confusionMatrix(factor(ifelse(model.glm.final.test > 0.5,"Yes","No")),y_test)

cm$table
cm$overall[1]
cm$overall[5]
```

We can see that accuracy (~97%) is approximately 3% better than the accuracy of our manually subsetted logistic regression model from above. We also see from the confusion table, that our True Negative rate for this model is (~98%), which is ~1% higher than the subsetted model above. Our True Positive rate in this model is ~81%, showing a dramatic increase of ~19 percentage points over the subsetted model above. 

We still want to see how well our model discriminates at different thresholds
```{r}
# ROC Curve
prob=plogis(predict.glm(model.glm.final, type = c("response")))
#head(prob)
h <- roc(Oscars_won_some~prob, data=oscars_train)
plot(h)
# Area Under the Curve
auc(h)
```
The ROC plot shows a definite improvement relative to the previous model, and the AUC is increased by ~3 percentage points (total of ~98%)

Lastly, let us cross-validate
```{r}
#FTrain
model.glm.final.fTrain <- function(train_data) {
  return (
    glm(Oscars_won_some~awards_wins + 
          Screen_Actors_Guild_won +
          Golden_Globes_won +
          genre_Drama +
          Writers_Guild_won +
          Online_Film_Television_Association_nominated +
          BAFTA_won,
        family = binomial(link = "logit"),
        data = train_data))
}

scores <- oscars %>% cross_validate( 
  fTrain = model.glm.final.fTrain, 
  fHitRate = model.glm.fHitRate)

scores
mean(scores)
sd(scores)
```

The average accuracy from the cross-validation approach (96.89%) is comparable to the accuracy shown in our model trained only from a single training set (96.93%). The standard deviation of the cross-validation accuracy is also quite low, suggesting that our model is not overfitting our data. 

We also need to confirm that we aren't seeing a massive amount of multicollinearity
```{r}
vif(model.glm.final)
```
Due to the large values for `award_wins` and `Online_Film_Television_Association_nominated`, we should probably repeat the stepwise regression process using a subset of the variables that are not the variables that are aggregated into `award_wins`, while keeping `award_wins` available for use by the stepwise variable selection process.

```{r}
oscars_train2 <- oscars_train %>% select(-matches(".*nominated$"), -matches(".*won$"))
oscars_test2 <- oscars_test %>% select(-matches(".*nominated$"), -matches(".*won$"))

# creating a null model
model.glm.null2 <-  glm(Oscars_won_some~1,
                family = binomial(link = "logit"), data = oscars_train2)
# create a full model
model.glm.all2 <- glm(Oscars_won_some~.,
                family = binomial(link = "logit"), data = oscars_train2)
```

```{r echo=TRUE}
# use step to apply stepwise forward selection with BIC as a evaluation metric
model.glm.final2 <- step(model.glm.null2, scope = formula(model.glm.all2), direction = "forward", k = log(nrow(oscars_train)), trace = FALSE)
# NOTE: The output says AIC, but we really are calculating BIC, because we are using the log of the number of observations in our dataset as our penalty term (k) instead of just the number of predictors. This seemed appropriate due to the large number of features relative the number of observations available to us, and BIC penalizes extra features more than AIC does.
```

```{r}
summary(model.glm.final2)
```

Restricting our variable selection to reduce the multicollinearity we saw before, has the step-wise variable selection giving us a final model with only 4 variables. We discovered that all of the 'non-Oscar'_nominated and 'non-Oscar'_won categories sum to the values in `awards_nominations` and `awards_wins` during the EDA process, therefore we realize that it is likely inappropriate to keep them all in the model while also including `awards_wins` and/or `awards_nominations`. `awards_wins` is still statistically significant in both models.

```{r}
model.glm.final2.test <- predict.glm(model.glm.final2,oscars_test2,type='response')
cm <- confusionMatrix(factor(ifelse(model.glm.final2.test > 0.5,"Yes","No")),y_test)
cm$table
cm$overall[1]
cm$overall[5]
```

Our Accuracy is still above the No Information Rate (AccuracyNull above), as with the previous model, but we did see a reduction of the overall prediction accuracy and true positive rates compared to the other step-wise model
```{r}
# ROC Curve
prob=plogis(predict.glm(model.glm.final2, type = c("response")))
#head(prob)
h <- roc(Oscars_won_some~prob, data=oscars_train)
plot(h)
# Area Under the Curve
auc(h)
```
AUC is lower than our initial forward selection model. However, the VIF statistic is improved relative to the previous model, as none of the variables have VIF scores above 2
```{r}
vif(model.glm.final2)
```
 
Finlally let us cross validate this model 
```{r}
# FTrain
model.glm.final2.fTrain <- function(train_data) {
  return(
    glm(Oscars_won_some~awards_wins +
          genre_Drama +
          genre_Biography +
          certificate, 
        family = binomial(link = "logit"),
        data = train_data)
  )
}

scores <- oscars %>% cross_validate(
  fTrain = model.glm.final2.fTrain,
  fHitRate = model.glm.fHitRate
)

scores
mean(scores)
sd(scores)
```
So apart from this being a worse model in predictive performance, it also has a higher variance. Thus, for now, Logistic Regression using the correlated variables gives the best results. 

## Random Forest

Now we will attempt to train a Random Forest model based on all the variables, and tune the hyperparameter `mtry` (the number of features used at each split).

```{r}
# apparently randomForest function can't use `-` in variable names.
oscars_train.renamed <- oscars_train %>% 
  mutate(genre_Sci_Fi = `genre_Sci-Fi`) %>% 
  select(-`genre_Sci-Fi`)
oscars_test.renamed <- oscars_test %>% 
  mutate(genre_Sci_Fi = `genre_Sci-Fi`) %>% 
  select(-`genre_Sci-Fi`)
oscars.renamed <- oscars %>% 
  mutate(genre_Sci_Fi = `genre_Sci-Fi`) %>% 
  select(-`genre_Sci-Fi`)

# Trains the model for each value of mtry
ranFor.train <- function(mtry, train.data) {
  set.seed(42)
  return(randomForest(formula=Oscars_won_some~.,
               importance=TRUE, 
               proximity=TRUE,
               mtry=mtry, 
               data=train.data))
}

# Tests the model
ranFor.hitrate <- function (model, test.data) {
  y_test_pred <- predict(model,test.data, type='response')
  return(sum(y_test_pred==test.data$Oscars_won_some)
         /length(test.data$Oscars_won_some))
}

# Tunes mtry using cross validation on the training set
# A list of the final mean cross-validated accuracies for each
# value of mtry

#scores = c()

# This is the same cross-val loop used on the function cross_validate
# But only on the training data

#folds <- KFold(oscars_train.renamed$Oscars_won_some, n = 3, 
#                 stratified = TRUE, seed = 42)
# Gets a list with the number of folds
#list_ = 1:length(folds)
#for (i in seq(1,80)) {
  # Gets a list to store the cross-val accuracies for different combination
  # of folds for training and testing
#  scoress = c()
  # For each fold
#  for(j in list_){
    # Gets the indexes for getting the training data
#    list_train = list_[-j]
#    train_index = c(folds[[list_train[1]]], folds[[list_train[2]]])
    # Gets the index for the testing data
#    test_index = folds[[j]]
    # Splits between training and testing
#    cv.train = oscars_train.renamed[train_index, ]
#    cv.test = oscars_train.renamed[test_index, ]
    # Standardizes the new data
#    scaleParam <- preProcess(cv.train, method=c("center", "scale"))
#    cv.train <- predict(scaleParam, cv.train)
#    cv.test <- predict(scaleParam, cv.test)
    # Trains the model for mtry=i
#    trained.model <- ranFor.train(i, cv.train)
    # Gets the cross-validated accuracies as a list
#    scoress <- c(scoress, ranFor.hitrate(trained.model, cv.test))
#  }
  # Appends to the final list the mean of the cross validated accuracies
  # for each mtry
#  scores = c(scores, mean(scoress))
#}
#which(max(scores)==scores)

# The code from above is commented out because it takes a while to run
# Its output is:
# [1]  47

# Let's try it 

best.ranFor.model = randomForest(formula=Oscars_won_some~.,
               importance=TRUE, 
               proximity=TRUE,
               mtry=47, 
               data=oscars_train.renamed)
y_test_pred <- predict(best.ranFor.model,oscars_test.renamed, type='response')
sum(y_test_pred==y_test)/length(y_test)

# confusion matrix
y_test_pred <- predict(best.ranFor.model,oscars_test.renamed, type='response')
confusionMatrix(y_test_pred, y_test)
```
Random Forest improves our prediction accuracy over the stepwise logistic regression model.

TODO: Mention that one might get different results depending on the computer...?!

TODO: Explain how and why we got a ROC Curve for RF. 
```{r}
#head(prob)
h <- roc(oscars_train.renamed$Oscars_won_some, best.ranFor.model$votes[, 2])
plot(h)
# Area Under the Curve
auc(h)
```


```{r}
#fTrain
model.ranFor.fTrain <- function(train_data) {
  set.seed(42)
  return(
    randomForest(
      formula=Oscars_won_some~.,
      importance=TRUE,
      proximity=TRUE,
      mtry=47,
      data=train_data))
}

#fHitRate
model.ranFor.fHitRate <- function(pred_y, orig_y) {
  a <- sum(pred_y==orig_y)
  b <- length(orig_y)
  return (a/b)
}

scores <- oscars.renamed %>% cross_validate(
  fTrain = model.ranFor.fTrain,
  fHitRate = model.ranFor.fHitRate
)

scores
mean(scores)
sd(scores)
```


```{r}
# Computes the importance of each variable
# by accuracy
VI_F1 = importance(best.ranFor.model, type=1)
# by impurity
VI_F2 = importance(best.ranFor.model, type=2)

# https://freakonometrics.hypotheses.org/19835
barplot(t(VI_F2/sum(VI_F2)), cex.names=0.5)
```


## Dealing with Imbalance

### Sampling Methods

[Source](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/)

```{r}
library(ROSE)
table(oscars_train.renamed$Oscars_won_some)
dim(oscars_train.renamed)

# Let's do oversampling
oscars_train_oversampled = ovun.sample(Oscars_won_some~., data=oscars_train.renamed, method="over", N=1594, seed=0)$data
table(oscars_train_oversampled$Oscars_won_some)
dim(oscars_train_oversampled)

# Let's test it!
sampling_test = function(data){
  model <- 
    glm(Oscars_won_some~awards_wins + 
            Screen_Actors_Guild_won +
            Golden_Globes_won +
            genre_Drama +
            Writers_Guild_won +
            Online_Film_Television_Association_nominated +
            BAFTA_won,
          family = binomial(link = "logit"),
          data = data)
  
      
  ypred_test <- predict.glm(model,
                                 oscars_test.renamed,type='response')
  return(confusionMatrix(factor(ifelse(ypred_test >
                                      0.5,"Yes","No")),y_test))
}

# No sampling
conf_matrix = sampling_test(oscars_train)
conf_matrix$overall[1]
conf_matrix$overall[5]

# Oversampling
conf_matrix = sampling_test(oscars_train_oversampled)
conf_matrix$table
conf_matrix$overall[1]
conf_matrix$overall[5]

# Well, we got only 1 false negative! But more false positives... 

# Undersampling
oscars_train_undersampled = ovun.sample(Oscars_won_some~., data=oscars_train.renamed, method="under", N=134, seed=0)$data
table(oscars_train_undersampled$Oscars_won_some)
conf_matrix = sampling_test(oscars_train_undersampled)
# I figured....!
conf_matrix$table
# Mmm, well we do need to cross-validate all of this!

# Both
oscars_train_under_over <- ovun.sample(Oscars_won_some ~ ., data = oscars_train.renamed, method = "both", p=0.5, seed=0)$data
table(oscars_train_under_over$Oscars_won_some)
conf_matrix = sampling_test(oscars_train_under_over)
conf_matrix$table
# Well..

# Synthetic
oscars_train_rose <- ROSE(Oscars_won_some~., data = oscars_train.renamed, seed = 0)$data
table(oscars_train_rose$Oscars_won_some)
conf_matrix = sampling_test(oscars_train_rose)
conf_matrix$table
# Shit, I had high hopes for this one
```

```{r}
# Let's try with Random Forest...
rf_sampling_test = function(data){
  model <- ranFor.train(47, data)
  y_test_pred <- predict(model,oscars_test.renamed,
                         type='response')
  return(confusionMatrix(y_test_pred, y_test))
}

# No sampling
conf_matrix = rf_sampling_test(oscars_train.renamed)
conf_matrix$table
conf_matrix$overall[1]
conf_matrix$overall[5]

# Oversampling
conf_matrix = rf_sampling_test(oscars_train_oversampled)
conf_matrix$table
conf_matrix$overall[1]
conf_matrix$overall[5]

# Undersampling
conf_matrix = rf_sampling_test(oscars_train_undersampled)
# Lol
conf_matrix$table

# Both
conf_matrix = rf_sampling_test(oscars_train_under_over)
conf_matrix$table

# Synthetic
conf_matrix = rf_sampling_test(oscars_train_rose)
conf_matrix$overall[1]
conf_matrix$overall[5]
# Well this one is somewhat decent... but nah. 
```

So now we could cross-validate using sampling methods, but it doesn't make much sense since we are not going to improve the Multicoll LogReg we got. One last thing we can do, is perform the sampling... and then run Stepwise Reg!

```{r}
test_step_sampling = function(data){
  # creating a null model
  model.glm.null <-  glm(Oscars_won_some~1,
                  family = binomial(link = "logit"), data = data)
  # create a full model
  model.glm.all <- glm(Oscars_won_some~.,
                  family = binomial(link = "logit"), data = data)
  
  model.glm.final <- step(model.glm.null, scope = formula(model.glm.all),
                          direction = "forward", k =
                            log(nrow(oscars_train_oversampled)), trace =
                            FALSE)
  
  model.glm.final.test <-
    predict.glm(model.glm.final,oscars_test,type='response')
  
  return(confusionMatrix(factor(ifelse(model.glm.final.test >
                                        0.5,"Yes","No")),y_test))
  }
# No sampling
conf_matrix = test_step_sampling(oscars_train)
conf_matrix$table
conf_matrix$overall[1]
conf_matrix$overall[5]

# Oversampling
conf_matrix = test_step_sampling(oscars_train_oversampled)
conf_matrix$table
conf_matrix$overall[1]
conf_matrix$overall[5]
#NOP!

# Undersampling
conf_matrix = test_step_sampling(oscars_train_undersampled)
conf_matrix$table
# Meh

# Both
conf_matrix = test_step_sampling(oscars_train_under_over)
conf_matrix$table
# :(

# Synthetic
conf_matrix = test_step_sampling(oscars_train_rose)
conf_matrix$table
# Nooop
```

So sampling methods are unsuccessful Such a shame.

### Weights balanced

```{r}
wrf = randomForest(formula=Oscars_won_some~.,
               importance=TRUE, 
               proximity=TRUE,
               mtry=47, 
               data=oscars_train.renamed, classwt = c(0.92, 0.08))
y_test_pred <- predict(wrf,oscars_test.renamed, type='response')
conf_matrix = confusionMatrix(y_test_pred, y_test)
conf_matrix$table
#NOP
```

No idea how to make this for Logistic Regression in R. Not even sure `classwt` is equivalent to `class_weight = balanced` in [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).  

So after some thought and the experimental results we got, balancing may not be needed in our case... See [this](https://stats.stackexchange.com/questions/164693/adding-weights-to-logistic-regression-for-imbalanced-data). 

# Conclusions 

